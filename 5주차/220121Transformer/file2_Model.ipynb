{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WPNN6BmXXYvd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import sys\n",
        "\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "# visualization 라이브러리\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p77g9I5hXYvi"
      },
      "source": [
        "# sample input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pJnFPHdaXYvi"
      },
      "outputs": [],
      "source": [
        "sample_input = torch.tensor([[    1,    57,  3518,   155,     6,   943, 10716, 20433,     0,     6,\n",
        "         12181,   362,     4,   588,   245,   243,   230,   241,   231,   240,\n",
        "           233,   238,   239,   232,     4,  1761,    20,     2,  5063,  1007,\n",
        "          2085,  2907,     4, 19454,   658,  5499,  8778,  1132,  4189,   283,\n",
        "             4,     4,    20,     2,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
        "\n",
        "sample_mask1 = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "\n",
        "sample_mask2 = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "\n",
        "\n",
        "sample_label = torch.tensor([0])\n",
        "\n",
        "sample = [sample_input,sample_mask2,sample_mask1,sample_label]\n",
        "\n",
        "sample_config = {\n",
        "    \"dim\": 768,\n",
        "    \"dim_ff\": 3072,\n",
        "    \"n_layers\": 12,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 12,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 512,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "model_config = AttributeDict(sample_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WUwodJWXYvm",
        "outputId": "9682bee9-e7f2-409b-b094-9327dae8066d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "sample_input.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyI_jPgqXYvo"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "he_iYTHsXYvp"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-5,5,30)\n",
        "plt.plot(x,gelu(x))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "j-a8Wwa84Mpr",
        "outputId": "9afe0bff-9cd0-467d-cf1c-6632f262abd2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n817XUUkXYvu"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6WDZJoniXYvv"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # 각 단어별 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # 각 단어별 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67vQ81wQXYv0"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5i9AMbZKXYv1"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
        "        self.seg_embed = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.drop(self.norm(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rvIadxVfXYv4"
      },
      "outputs": [],
      "source": [
        "model = Embeddings(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlkPorndXYv7",
        "outputId": "2698c342-bda2-4319-cb60-d04e178863cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "out = model(*sample[0:2])\n",
        "out.size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI2c7XzQ29d2",
        "outputId": "007fe82d-cc8f-4c3f-85b0-6deb0832aaa6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1,    57,  3518,   155,     6,   943, 10716, 20433,     0,     6,\n",
              "         12181,   362,     4,   588,   245,   243,   230,   241,   231,   240,\n",
              "           233,   238,   239,   232,     4,  1761,    20,     2,  5063,  1007,\n",
              "          2085,  2907,     4, 19454,   658,  5499,  8778,  1132,  4189,   283,\n",
              "             4,     4,    20,     2,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VlEHDC_29d3",
        "outputId": "5cfd04ec-1901-45a3-938f-e3832b73bac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3233,  1.3021,  0.0000,  ...,  1.2345,  1.3064,  1.1453],\n",
              "        [-1.6845,  1.1804,  0.8632,  ..., -0.0000,  1.2621,  0.3760],\n",
              "        [-0.9437, -1.5301,  1.1847,  ...,  0.0872,  0.9528,  0.3047],\n",
              "        [-0.9913,  0.3079,  1.3523,  ..., -0.0000,  1.3691,  0.1057],\n",
              "        [-1.4005,  0.8539,  2.0624,  ..., -0.8189,  0.7783,  0.5539]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "out[0,-5:] # 뒤의 5개는 모두 pad 토큰인데 embedding 값이 다릅니다. 그 이유가 뭘까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPe3CSN0XYwC"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MHiJimv_XYwD"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "                 / math.sqrt(query.size(-1)) # scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CK91Q-b3XYwF"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        B,S,D = x.shape\n",
        "\n",
        "\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        \n",
        "        q, k, v = (x.reshape(B,S,self.n_heads,-1).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = (scores @ v).transpose(1, 2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = h.reshape(B,S,D)\n",
        "        self.scores = scores\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRTC9578XYwI"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "luh9VPi4XYwJ"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fRUW4DeXYwM"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "u0HTaKSgXYwM"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadedSelfAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.embed = Embeddings(cfg)\n",
        "        self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "    def forward(self, x, seg, mask):\n",
        "        h = self.embed(x, seg)\n",
        "        for block in self.blocks:\n",
        "            h = block(h, mask)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii-ll7BbXYwP",
        "outputId": "bd976f36-b91b-4633-a4a0-d96d8dcb7ed3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = Transformer(model_config)\n",
        "out = model(*sample[:3])\n",
        "out.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xRq3zOSy29d5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5gilSNpXYwT"
      },
      "source": [
        "# BERT (for classification only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nfQDGyTjXYwT"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\" Classifier with Transformer \"\"\"\n",
        "    def __init__(self, cfg, n_labels):\n",
        "        super().__init__()\n",
        "        self.transformer = Transformer(cfg)\n",
        "        self.fc = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        self.classifier = nn.Linear(cfg.dim, n_labels)\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, input_mask):\n",
        "        h = self.transformer(input_ids, segment_ids, input_mask)\n",
        "        # only use the first h in the sequence\n",
        "        pooled_h = self.activ(self.fc(h[:, 0]))\n",
        "        logits = self.classifier(self.drop(pooled_h))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7g9odreXYwW",
        "outputId": "e20c67be-eb51-43a8-f9f0-2810ac9410ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model = Classifier(model_config, 2)\n",
        "out = model(*sample[:3])\n",
        "out.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2dzZMGpXYwY",
        "outputId": "73bc6599-1bde-4e31-8144-599700bd7b56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0430, 0.2718]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kDor1UHx6Af8"
      },
      "execution_count": 19,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "file2_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}