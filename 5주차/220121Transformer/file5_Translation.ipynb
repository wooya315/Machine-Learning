{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "file5_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFyZ5vetQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b29ccc-3f3d-4193-c5e7-1ed71c337d3e"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tqdm\n",
        "from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도\n",
        "from random import choice, randrange # random\n",
        "from itertools import zip_longest \n",
        "import json \n",
        "import random\n",
        "import pdb\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "ABHpkPXsV-Gx",
        "outputId": "87fe7126-e0ba-4d1a-ee40-84db6a341a68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-887aa095-5aed-46fa-b3a5-59956ea31c37\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-887aa095-5aed-46fa-b3a5-59956ea31c37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_files.zip to data_files.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-eRAdsqWanC",
        "outputId": "70b65c74-0ac1-46bd-85ca-4438c77b7737"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data_files.zip\n",
            "  inflating: english_id2word.pkl     \n",
            "  inflating: english_vocab.pkl       \n",
            "  inflating: english_word2id.pkl     \n",
            "   creating: korean_data/\n",
            "  inflating: korean_data/test_english.csv  \n",
            "  inflating: korean_data/test_korean.csv  \n",
            "  inflating: korean_data/train.csv   \n",
            "  inflating: korean_id2word.pkl      \n",
            "  inflating: korean_vocab.pkl        \n",
            "  inflating: korean_word2id.pkl      \n",
            "  inflating: train_english.pkl       \n",
            "  inflating: train_korean.pkl        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYCLJa4petQg"
      },
      "source": [
        "def split_last(x, shape):\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQR9lBMetQh"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295zs7CWetQj"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWS8voZZetQk"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYgHC0ietQk"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # sequence 방향 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # sequence 방향 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiPES9getQl"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbTPl6aSetQm"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embed = nn.Embedding(vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(512, cfg.dim),freeze=True) # position embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsNAdbnetQn"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ulMozSetQn"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1)) # scale\n",
        "        \n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDtvhua3etQo"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask, x_q=None):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        \n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        if x_q is None:\n",
        "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        else:\n",
        "            q, k, v = self.proj_q(x_q), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "            scores = scores.masked_fill_(mask, -1e9)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = torch.matmul(scores, v).transpose(1,2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQUnmexetQo"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMI83XbetQo"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgFSAOGNetQp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEoqWUFetQp"
      },
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "    \n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "    \n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequent_mask = torch.tensor(subsequent_mask, device=seq.device).byte()\n",
        "    return subsequent_mask\n",
        "    \n",
        "    \n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(cfg)\n",
        "        self.encoder_attention = MultiHeadAttention(cfg)\n",
        "        \n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.proj1 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.proj2 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        \n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm3 = LayerNorm(cfg)\n",
        "        \n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        \n",
        "    def forward(self,x , enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \n",
        "        # self-attention -> add&norm\n",
        "        h = self.self_attention(x, dec_self_attn_mask)\n",
        "        h = self.norm1(x + self.drop(self.proj1(h)))\n",
        "        \n",
        "        # encoder attention -> add&norm\n",
        "        h2 = self.encoder_attention(enc_outputs, dec_enc_attn_mask, x_q=h)\n",
        "        h = self.norm2(h + self.drop(self.proj2(h2))) \n",
        "        \n",
        "        # feedforward network\n",
        "        h = self.norm3(h + self.drop(self.pwff(h)))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        #====================encoder===========================\n",
        "        self.encoder_embed = Embeddings(cfg, len(korean_vocab))\n",
        "        self.encoder_blocks = nn.ModuleList([Encoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        #====================decoder============================\n",
        "        self.decoder_embed = Embeddings(cfg, len(english_vocab))\n",
        "        self.decoder_blocks = nn.ModuleList([Decoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "        \n",
        "        #=========================================================\n",
        "        self.projection = nn.Linear(cfg.dim, len(english_vocab))\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        #============encoder============\n",
        "        h = self.encoder_embed(enc_inputs)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h, enc_self_attn_mask)\n",
        "            \n",
        "        enc_outputs = h\n",
        "        \n",
        "        \n",
        "        #============decoder============\n",
        "        \n",
        "        # self attention mask\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).float()\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs).float()\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        # encoder attention mask\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        \n",
        "        \n",
        "        # embedding\n",
        "        h = self.decoder_embed(dec_inputs)\n",
        "        \n",
        "        \n",
        "        for block in self.decoder_blocks:\n",
        "            h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "        #============projection==========\n",
        "        \n",
        "        out = self.projection(h)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def greedy_decoding(self, enc_inputs, start_token_index = 1, end_token_index = 2, generation_max_len=128):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            batch_size, max_length = enc_inputs.size()\n",
        "            generation_end_flag = [0 for i in range(batch_size)]\n",
        "            predicted_sentences = []\n",
        "            #=================encoding=============\n",
        "            h = self.encoder_embed(enc_inputs)\n",
        "            enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "            for block in self.encoder_blocks:\n",
        "                h = block(h, enc_self_attn_mask)    \n",
        "            enc_outputs = h\n",
        "\n",
        "            #================ greedy decoding ==================\n",
        "            # dec_inputs : (batch size, 1) \n",
        "            dec_inputs = torch.ones(batch_size, 1, device=enc_inputs.device) * start_token_index\n",
        "            dec_inputs = dec_inputs.long()\n",
        "\n",
        "            for i in range(generation_max_len):\n",
        "\n",
        "                #====================== decoder =======================\n",
        "                # self attention mask\n",
        "                dec_self_attn_pad_mask = None\n",
        "                dec_self_attn_subsequent_mask = None\n",
        "                dec_self_attn_mask = None\n",
        "\n",
        "                # encoder attention mask\n",
        "                dec_enc_attn_mask = None\n",
        "\n",
        "\n",
        "                # embedding\n",
        "                h = self.decoder_embed(dec_inputs)\n",
        "\n",
        "\n",
        "                for block in self.decoder_blocks:\n",
        "                    h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "\n",
        "\n",
        "                out = self.projection(h[:,-1,:])\n",
        "                pred = out.argmax(-1) \n",
        "\n",
        "#                 print(out.size(), pred)\n",
        "\n",
        "                dec_inputs = torch.cat((dec_inputs, pred.unsqueeze(1)),dim=1)\n",
        "    #             print(dec_inputs)\n",
        "\n",
        "                predicted_sentences.append(pred)\n",
        "                for j, boolean in enumerate(pred==end_token_index):\n",
        "                    if boolean == True:\n",
        "                        generation_end_flag[j] = 1\n",
        "                if sum(generation_end_flag) == batch_size:\n",
        "                    break\n",
        "\n",
        "        return torch.stack(predicted_sentences, dim=1)\n",
        "        \n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmYe4SsTetQq"
      },
      "source": [
        "# 한 -> 영\n",
        "data_ = pd.read_csv(\"./korean_data/train.csv\")\n",
        "korean_data = data_[\"Korean\"].values\n",
        "english_data = data_[\"English\"].values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRZJnw7CetQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f609ce-86a0-43af-a2d5-a2201b0a8748"
      },
      "source": [
        "nltk.tokenize.word_tokenize(korean_data[100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['그것을', '막기', '위해', ',', '많은', '과학자는', '친환경적인', '사용', '방법을', '연구했어요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weR-02iWetQq"
      },
      "source": [
        "def build_dict(seqs):\n",
        "    num_skip_sent = 0\n",
        "    word_count = 4\n",
        "    vocab = [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"]\n",
        "    word2id = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
        "    id2word = {0: \"<pad>\", 1: \"<s>\", 2: \"</s>\", 3: \"<unk>\"}\n",
        "    print(\"Building vocab and dict..\")\n",
        "    for line in seqs:\n",
        "        words = line.strip().split(' ') # tokenized by space \n",
        "        for word in words:\n",
        "            if word not in vocab:\n",
        "                word_count += 1 # increment word_count\n",
        "                vocab.append(word) # append new unique word\n",
        "                index = word_count - 1 # word index (consider index 0)\n",
        "                word2id[word] = index # word to index\n",
        "                id2word[index] = word # index to word\n",
        "    \n",
        "    print(\"Number of unique words: %d\" % len(vocab))\n",
        "    print(\"Finised building vocab and dict!\")\n",
        "\n",
        "    return vocab, word2id, id2word"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "et5ZhXWOYS82"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBX1j41etQr"
      },
      "source": [
        "if os.path.isfile(\"./korean_vocab.pkl\"):\n",
        "    with open(\"./train_korean.pkl\", \"rb\") as f:\n",
        "        korean_data_token_ = pickle.load(f)\n",
        "    with open(\"./train_english.pkl\", \"rb\") as f:\n",
        "        english_data_token_ = pickle.load(f)    \n",
        "    \n",
        "    with open(\"./korean_vocab.pkl\", \"rb\") as f:\n",
        "        korean_vocab = pickle.load(f)\n",
        "    with open(\"./korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    with open(\"./korean_id2word.pkl\", \"rb\") as f:\n",
        "        korean_id2word = pickle.load(f)\n",
        "    with open(\"./english_vocab.pkl\", \"rb\") as f:\n",
        "        english_vocab = pickle.load(f)\n",
        "    with open(\"./english_word2id.pkl\", \"rb\") as f:\n",
        "        english_word2id = pickle.load(f)\n",
        "    with open(\"./english_id2word.pkl\", \"rb\") as f:\n",
        "        english_id2word = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    korean_data_token = []\n",
        "    for sent in korean_data:\n",
        "        korean_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    english_data_token = []\n",
        "    for sent in english_data:\n",
        "        english_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    korean_data_token_ = [' '.join(token) for token in korean_data_token]\n",
        "    english_data_token_ = [' '.join(token) for token in english_data_token]    \n",
        "    \n",
        "    korean_vocab, korean_word2id, korean_id2word = build_dict(korean_data_token_)\n",
        "    english_vocab, english_word2id, english_id2word = build_dict(english_data_token_)\n",
        "    \n",
        "    pickle.dump(korean_data_token_, open(\"./train_korean.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_data_token_, open(\"./train_english.pkl\", \"wb\" ))    \n",
        "    \n",
        "    pickle.dump(korean_vocab, open(\"./korean_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_word2id, open(\"./korean_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_id2word, open(\"./korean_id2word.pkl\", \"wb\" ))\n",
        "\n",
        "    pickle.dump(english_vocab, open(\"./english_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_word2id, open(\"./english_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_id2word, open(\"./english_id2word.pkl\", \"wb\" ))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIP8QaNetQr"
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args)\n",
        "\n",
        "\n",
        "def pad_tensor(vec, pad, value=0, dim=0):\n",
        "    \"\"\"\n",
        "    pad token으로 채우는 용도 \n",
        "    args:\n",
        "        vec - tensor to pad\n",
        "        pad - the size to pad to\n",
        "        dim - dimension to pad\n",
        "    return:\n",
        "        a new tensor padded to 'pad' in dimension 'dim'\n",
        "    \"\"\"\n",
        "    pad_size = pad - vec.shape[0]\n",
        "\n",
        "    if len(vec.shape) == 2:\n",
        "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
        "    elif len(vec.shape) == 1:\n",
        "        zeros = torch.ones((pad_size,)) * value\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKU7nf9etQr"
      },
      "source": [
        "def pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
        "    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
        "    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
        "    ys = torch.stack([x[1] for x in batch], dim=0)\n",
        "    xs = xs[xids].contiguous() # decreasing order로 다시 나열 \n",
        "    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n",
        "    target_lengths = target_lengths[xids] \n",
        "    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int()\n",
        "\n",
        "\n",
        "class ToyDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    https://talbaumel.github.io/blog/attention/\n",
        "    \"\"\"\n",
        "    def __init__(self,  ko_path, en_path , ko_word2id, en_word2id):\n",
        "        with open(ko_path, \"rb\") as f:\n",
        "            self.ko_seqs = pickle.load(f)\n",
        "        with open(en_path, \"rb\") as f:\n",
        "            self.en_seqs = pickle.load(f)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        self.en_word2id = en_word2id\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        en_seqs = self.en_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        en_seqs = self.process(en_seqs, self.en_word2id)\n",
        "        return ko_seqs, en_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"])\n",
        "        words = seq.strip().split(' ')\n",
        "        for word in words:\n",
        "            if len(sequence) < model_config.max_len:\n",
        "                if word in word2id:\n",
        "                    sequence.append(word2id[word]) # \n",
        "                else:\n",
        "                    sequence.append(3) # replace by <unk> token\n",
        "            else:\n",
        "                break\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN1gCKHpetQs"
      },
      "source": [
        "sample_config = {\n",
        "    \"dim\": 32,\n",
        "    \"dim_ff\": 32,\n",
        "    \"n_layers\": 2,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 4,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 30,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522,\n",
        "    \"batch_size\": 32\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "model_config = AttributeDict(sample_config)\n",
        "model = Transformer(model_config)\n",
        "model = model.cuda()\n",
        "# out = model(sample[0].cuda(),sample[0].cuda())\n",
        "# out.size()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XST7y_CDetQs"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Io-Gd9etQs"
      },
      "source": [
        "dataset = ToyDataset(\"train_korean.pkl\", \"train_english.pkl\", korean_word2id, english_word2id)\n",
        "train_loader = data.DataLoader(dataset, batch_size=model_config.batch_size, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w17hIP50etQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df96245a-fdfa-443b-8698-4af8c4f52308"
      },
      "source": [
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    loss_list =[]\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        x, y, x_len, y_len = batch\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        logits = model(x, y[:,:-1])\n",
        "        loss = loss_fn(logits.view(-1,len(english_vocab)) , y[:,1:].contiguous().view(-1)) # loss 구하기 우리는 cross entropy 사용 \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.item())\n",
        "        \n",
        "        if (idx+1) % 400 == 0 :\n",
        "            print('epoch {} iteration {}/{} loss {:.4f}'.format(epoch+1, idx+1, len(train_loader), np.mean(loss_list)))\n",
        "            loss_list=[]\n",
        "            \n",
        "        if (idx+1) % 2000 == 0:\n",
        "            with torch.no_grad():\n",
        "                greedy_pred = model.greedy_decoding(x[0].unsqueeze(0))\n",
        "            greedy_pred_indices = greedy_pred[0].data.cpu().tolist()\n",
        "            pred_indices = logits[0].argmax(-1).data.cpu().tolist()\n",
        "            input_indices = x[0].data.cpu().tolist()\n",
        "            label_indices = y[0].data.cpu().tolist()\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                greedy_pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                greedy_pred_len = 128\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            if 2 in label_indices:\n",
        "                label_len = label_indices.index(2)\n",
        "            else:\n",
        "                label_len = 128\n",
        "            \n",
        "            greedy_pred_words = [english_id2word[idx] for i, idx in enumerate(greedy_pred_indices) if i<= greedy_pred_len]\n",
        "            pred_words = [english_id2word[idx] for i, idx in enumerate(pred_indices) if i<=pred_len]\n",
        "            input_words = [korean_id2word[idx] for i, idx in enumerate(input_indices) if i<=input_len]\n",
        "            label_words = [english_id2word[idx] for i, idx in enumerate(label_indices) if i<=label_len]\n",
        "\n",
        "            print('=====================================')\n",
        "            print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "            print('출력(teacher forcing):{}'.format(' '.join(pred_words[:-1])))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(greedy_pred_words[:-1])))\n",
        "            print('정답:{}'.format(' '.join(label_words[1:-1])))\n",
        "            print('=====================================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 iteration 400/11250 loss 6.9043\n",
            "epoch 1 iteration 800/11250 loss 5.7146\n",
            "epoch 1 iteration 1200/11250 loss 5.4468\n",
            "epoch 1 iteration 1600/11250 loss 5.2941\n",
            "epoch 1 iteration 2000/11250 loss 5.1907\n",
            "=====================================\n",
            "입력:지금 DHL로 당신이 저희 쪽으로 패키지 발송할 때 주의라고만 적어서 발송되고 있는데 발송 시 발송인에 꼭 당신의 이름을 적어주세요 .\n",
            "출력(teacher forcing):I I have the good the be , the the you is a have you to lot . I you the . the . house . the same to\n",
            "출력(greedy decoding):I think you can not be be a lot of the same .\n",
            "정답:When you send a package to us via DHL , it 's just sending us a reminder so please make sure to write your name on the sender when\n",
            "=====================================\n",
            "epoch 1 iteration 2400/11250 loss 5.1170\n",
            "epoch 1 iteration 2800/11250 loss 5.0593\n",
            "epoch 1 iteration 3200/11250 loss 5.0131\n",
            "epoch 1 iteration 3600/11250 loss 4.9645\n",
            "epoch 1 iteration 4000/11250 loss 4.9440\n",
            "=====================================\n",
            "입력:제 꿈은 제 아이들이 피부색을 기준으로 사람을 평가하지 않고 인격을 기준으로 사람을 평가하는 나라에서 살 게 되는 거예요 .\n",
            "출력(teacher forcing):I will been reason to I friend is be of , , the lot , the is the be . the same . the own . I same .\n",
            "출력(greedy decoding):I think you can be a new best and I have to be a lot of the best and the most .\n",
            "정답:I have a dream that my children will one day live in a nation where people are not judged by the color of their skin but the content of\n",
            "=====================================\n",
            "epoch 1 iteration 4400/11250 loss 4.8982\n",
            "epoch 1 iteration 4800/11250 loss 4.8470\n",
            "epoch 1 iteration 5200/11250 loss 4.8552\n",
            "epoch 1 iteration 5600/11250 loss 4.8102\n",
            "epoch 1 iteration 6000/11250 loss 4.7950\n",
            "=====================================\n",
            "입력:우수한 수준의 호주 장애인 교육을 체험하기 위해 방문 기관을 선정 하던 중 귀 기관으로 연락드리게 되었습니다 .\n",
            "출력(teacher forcing):The 'm not that to the same , the of order and the the the and . own the of the same .\n",
            "출력(greedy decoding):The reason is the most important of the world , and the world , and the same .\n",
            "정답:I am contacting you in the process of selecting an institution to visit to experience Australia 's excellent education for the disabled .\n",
            "=====================================\n",
            "epoch 1 iteration 6400/11250 loss 4.7691\n",
            "epoch 1 iteration 6800/11250 loss 4.7547\n",
            "epoch 1 iteration 7200/11250 loss 4.7640\n",
            "epoch 1 iteration 7600/11250 loss 4.7490\n",
            "epoch 1 iteration 8000/11250 loss 4.7028\n",
            "=====================================\n",
            "입력:아사다 마오는 자신의 은퇴에 관해 큰 결단을 내렸지만 , 이 또한 인생에서의 하나의 통과점이라고 생각한다고 했습니다 .\n",
            "출력(teacher forcing):The a of that the , been to and lot time , I year of a of the people , can been the . the\n",
            "출력(greedy decoding):The most of the same thing , and the same , the product is the same time to the same time .\n",
            "정답:Asada Mao said that she has just made a big decision but this one is one of many things she has to pass through .\n",
            "=====================================\n",
            "epoch 1 iteration 8400/11250 loss 4.6987\n",
            "epoch 1 iteration 8800/11250 loss 4.6978\n",
            "epoch 1 iteration 9200/11250 loss 4.6680\n",
            "epoch 1 iteration 9600/11250 loss 4.6629\n",
            "epoch 1 iteration 10000/11250 loss 4.6386\n",
            "=====================================\n",
            "입력:예를 들어 깊은 기초들과 같은 파란 건물들의 배선 폭은 지진으로부터 건물을 보호할 수 있습니다 .\n",
            "출력(teacher forcing):The the , the product , the , the , is and as the and . and be the . the .\n",
            "출력(greedy decoding):The most of the most of the company is the same year of the world can be used .\n",
            "정답:For example , the width of wiring in blue buildings , such as deep foundations , can protect buildings from earthquakes .\n",
            "=====================================\n",
            "epoch 1 iteration 10400/11250 loss 4.6351\n",
            "epoch 1 iteration 10800/11250 loss 4.6112\n",
            "epoch 1 iteration 11200/11250 loss 4.6178\n",
            "epoch 2 iteration 400/11250 loss 4.5106\n",
            "epoch 2 iteration 800/11250 loss 4.5064\n",
            "epoch 2 iteration 1200/11250 loss 4.4984\n",
            "epoch 2 iteration 1600/11250 loss 4.4668\n",
            "epoch 2 iteration 2000/11250 loss 4.4778\n",
            "=====================================\n",
            "입력:월초에 가장 많은 데이터 변동이 발생한다면 월초 데이터 로드 이후 가장 이른 시간 내에 데이터를 확인할 수 있길 바랍니다 .\n",
            "출력(teacher forcing):It you , a , same of the same , the new , I can to be the product to a as a to it same . a to\n",
            "출력(greedy decoding):If you have to use the best , the same time , I can be able to get a good idea of the same time to the same time\n",
            "정답:If data is changed the most at the beginning of a month , I want to check the data as soon as possible once the data is loaded at\n",
            "=====================================\n",
            "epoch 2 iteration 2400/11250 loss 4.4837\n",
            "epoch 2 iteration 2800/11250 loss 4.4604\n",
            "epoch 2 iteration 3200/11250 loss 4.4791\n",
            "epoch 2 iteration 3600/11250 loss 4.4335\n",
            "epoch 2 iteration 4000/11250 loss 4.4348\n",
            "=====================================\n",
            "입력:제주 관광의 메카 , 중문과도 불과 15분 거리이니 , 조용하면서도 편리한 위치라고 할 수 있어요 .\n",
            "출력(teacher forcing):The , I Island , , , , and a the , , , and I , not to the . is own . , a in the\n",
            "출력(greedy decoding):The reason is a lot of the world , and the world , and the world can be used .\n",
            "정답:Jungmun , Jeju travel 's must-visit place , is only 15 minutes away , so it is likely to say that our guest house is located in a quiet\n",
            "=====================================\n",
            "epoch 2 iteration 4400/11250 loss 4.4380\n",
            "epoch 2 iteration 4800/11250 loss 4.4354\n",
            "epoch 2 iteration 5200/11250 loss 4.4353\n",
            "epoch 2 iteration 5600/11250 loss 4.4133\n",
            "epoch 2 iteration 6000/11250 loss 4.3991\n",
            "=====================================\n",
            "입력:교과서에서 ‘ 가정법 과거 ' 문장이 자주 등장하다 보니 , 나는 ‘ 가정법 과거 ' 에 대한 다양한 문장을 접할 수 있었습니다 .\n",
            "출력(teacher forcing):I the most , , the most , , , , , the , I can able to the , people . the . the world . . .\n",
            "출력(greedy decoding):I think the most important thing that I can become a lot of the world .\n",
            "정답:As the phrase 'History of the Family Law ' appears frequently in textbooks , I was able to access various sentences about 'History of the Family Law ' .\n",
            "=====================================\n",
            "epoch 2 iteration 6400/11250 loss 4.3979\n",
            "epoch 2 iteration 6800/11250 loss 4.4072\n",
            "epoch 2 iteration 7200/11250 loss 4.3834\n",
            "epoch 2 iteration 7600/11250 loss 4.3535\n",
            "epoch 2 iteration 8000/11250 loss 4.3705\n",
            "=====================================\n",
            "입력:왜 이제 와서 바쁘기 때문에 이 문제에 대해 신경 쓰기 어렵다고 하는 건지 이해할 수가 없네요 .\n",
            "출력(teacher forcing):I think n't know the I , can me to I is not to be a of the . . I . not .\n",
            "출력(greedy decoding):I think I can not make a little bit of the same .\n",
            "정답:I do n't understand why now you telling me that it 's hard to take care of this problem because you are busy .\n",
            "=====================================\n",
            "epoch 2 iteration 8400/11250 loss 4.3871\n",
            "epoch 2 iteration 8800/11250 loss 4.3601\n",
            "epoch 2 iteration 9200/11250 loss 4.3599\n",
            "epoch 2 iteration 9600/11250 loss 4.3583\n",
            "epoch 2 iteration 10000/11250 loss 4.3383\n",
            "=====================================\n",
            "입력:그렇기 때문에 파울은 아마도 그 위험한 결함이 실질적인 , 없으면 피해가 일어나지 않았을 , 원인이었다고 주장할 것입니다 .\n",
            "출력(teacher forcing):Therefore , I is not to the I one , , not same , but , of the life ,\n",
            "출력(greedy decoding):I think it is the same time , but I can not be able to the\n",
            "정답:Therefore , Paul can likely argue that the dangerous defect was the actual , but-for cause of his harm .\n",
            "=====================================\n",
            "epoch 2 iteration 10400/11250 loss 4.3563\n",
            "epoch 2 iteration 10800/11250 loss 4.3382\n",
            "epoch 2 iteration 11200/11250 loss 4.3372\n",
            "epoch 3 iteration 400/11250 loss 4.2545\n",
            "epoch 3 iteration 800/11250 loss 4.2589\n",
            "epoch 3 iteration 1200/11250 loss 4.2376\n",
            "epoch 3 iteration 1600/11250 loss 4.2360\n",
            "epoch 3 iteration 2000/11250 loss 4.2464\n",
            "=====================================\n",
            "입력:예를 들면 , 제가 생각했던 것보다 더워서 , 대학으로 가고 싶은 생각이 안 들어요 .\n",
            "출력(teacher forcing):I the , I , not , the could , but I I 'm n't know like to to be ,\n",
            "출력(greedy decoding):I 'm sorry , but I do n't want to do n't know why I do .\n",
            "정답:For example , it is hotter than I imagined , so that I do n't feel like going to university .\n",
            "=====================================\n",
            "epoch 3 iteration 2400/11250 loss 4.2371\n",
            "epoch 3 iteration 2800/11250 loss 4.2369\n",
            "epoch 3 iteration 3200/11250 loss 4.2497\n",
            "epoch 3 iteration 3600/11250 loss 4.2113\n",
            "epoch 3 iteration 4000/11250 loss 4.2290\n",
            "=====================================\n",
            "입력:초등학생 때 온 이후로 처음 와 보는데 역사에 대해서 더 알고 오니 더 잘 이해할 수 있었다 .\n",
            "출력(teacher forcing):I was have the if I have the the , but the I was , the the friends school . .\n",
            "출력(greedy decoding):I have been able to know if you have to know the time of the school .\n",
            "정답:I could understand better since I learned about history , unlike when I first visited in my elementary school years .\n",
            "=====================================\n",
            "epoch 3 iteration 4400/11250 loss 4.2314\n",
            "epoch 3 iteration 4800/11250 loss 4.2232\n",
            "epoch 3 iteration 5200/11250 loss 4.2217\n",
            "epoch 3 iteration 5600/11250 loss 4.1958\n",
            "epoch 3 iteration 6000/11250 loss 4.2168\n",
            "=====================================\n",
            "입력:지난번에 당신이 한국을 방문했을 때 확인했지만 이미 반도체 산업용 펌프의 모터는 BLDC 타입으로 대부분 변경되었습니다 .\n",
            "출력(teacher forcing):I you have the the company time Korea the , I first of the first , of of , been completed . Korea . of\n",
            "출력(greedy decoding):The product you sent to the company 's own and the same time , we are already sent .\n",
            "정답:As you checked during your last visit to Korea , the motors for the semiconductor industry pump already have been changed to BLDC type .\n",
            "=====================================\n",
            "epoch 3 iteration 6400/11250 loss 4.2137\n",
            "epoch 3 iteration 6800/11250 loss 4.1994\n",
            "epoch 3 iteration 7200/11250 loss 4.2123\n",
            "epoch 3 iteration 7600/11250 loss 4.2099\n",
            "epoch 3 iteration 8000/11250 loss 4.1934\n",
            "=====================================\n",
            "입력:저는 이러한 능력이 많은 사람의 공감을 얻을 수 있는 인문학적인 광고를 만드는 데 도움이 될 것이라고 자신해요 .\n",
            "출력(teacher forcing):I have make that the is to be you more to use it a . they be it people .\n",
            "출력(greedy decoding):I can make a lot of the job and the best thing to do you .\n",
            "정답:I can ensure that this ability would give much help to make humanistic advertisements that can attract many people .\n",
            "=====================================\n",
            "epoch 3 iteration 8400/11250 loss 4.2205\n",
            "epoch 3 iteration 8800/11250 loss 4.1963\n",
            "epoch 3 iteration 9200/11250 loss 4.2060\n",
            "epoch 3 iteration 9600/11250 loss 4.1946\n",
            "epoch 3 iteration 10000/11250 loss 4.1831\n",
            "=====================================\n",
            "입력:저는 이 영화를 보고 과거에 일어났던 끔찍한 일들을 잊어서는 안 된다고 생각했습니다 .\n",
            "출력(teacher forcing):I think this I have like go to movie and that to the movie to the the movie .\n",
            "출력(greedy decoding):I thought this movie and I was not a movie .\n",
            "정답:I thought that we would never forget the terrible things happened in the past after seeing this movie .\n",
            "=====================================\n",
            "epoch 3 iteration 10400/11250 loss 4.1984\n",
            "epoch 3 iteration 10800/11250 loss 4.1947\n",
            "epoch 3 iteration 11200/11250 loss 4.1922\n",
            "epoch 4 iteration 400/11250 loss 4.0896\n",
            "epoch 4 iteration 800/11250 loss 4.1050\n",
            "epoch 4 iteration 1200/11250 loss 4.1092\n",
            "epoch 4 iteration 1600/11250 loss 4.1024\n",
            "epoch 4 iteration 2000/11250 loss 4.0924\n",
            "=====================================\n",
            "입력:당신이 저에게 보낸 아래의 메일에 따라 , 당신이 일한 작업 시간을 타임 시트에 아래와 같이 입력해두었습니다 .\n",
            "출력(teacher forcing):I you 's you the invoice , are , to , please will the attached , sent in the invoice . soon sent . .\n",
            "출력(greedy decoding):I will send you the email to the mail and the other schedule .\n",
            "정답:As it said in the mail you sent me below , I entered the time you worked on the timesheet as you see below .\n",
            "=====================================\n",
            "epoch 4 iteration 2400/11250 loss 4.0939\n",
            "epoch 4 iteration 2800/11250 loss 4.1050\n",
            "epoch 4 iteration 3200/11250 loss 4.0795\n",
            "epoch 4 iteration 3600/11250 loss 4.0908\n",
            "epoch 4 iteration 4000/11250 loss 4.0854\n",
            "=====================================\n",
            "입력:예를 들어 더운 여름에는 삼계탕을 먹거나 , 비 오는 날에는 파전을 먹는 것들이 매우 흥미로웠다고 합니다 .\n",
            "출력(teacher forcing):The the , the was a a , the , , a , , the days , .\n",
            "출력(greedy decoding):I have a lot of a lot of the weather , and I 'm a good time .\n",
            "정답:For instance , she found eating samgyetang in hot summers or eating pajeon in rainy days interesting .\n",
            "=====================================\n",
            "epoch 4 iteration 4400/11250 loss 4.0962\n",
            "epoch 4 iteration 4800/11250 loss 4.0985\n",
            "epoch 4 iteration 5200/11250 loss 4.0895\n",
            "epoch 4 iteration 5600/11250 loss 4.0959\n",
            "epoch 4 iteration 6000/11250 loss 4.0955\n",
            "=====================================\n",
            "입력:내가 취업하면 , 아빠가 내 돈을 가져가실 거고 내 은행 계좌랑 다른 모든 걸 죄다 알게 되실 거야 .\n",
            "출력(teacher forcing):I I have a person , I , be a the best . I is be what though best . . let .\n",
            "출력(greedy decoding):I will have to know the best to see you to see my daughter .\n",
            "정답:Once I get a job , dad will take over my money and he will know even my bank account and everything .\n",
            "=====================================\n",
            "epoch 4 iteration 6400/11250 loss 4.0895\n",
            "epoch 4 iteration 6800/11250 loss 4.1000\n",
            "epoch 4 iteration 7200/11250 loss 4.0811\n",
            "epoch 4 iteration 7600/11250 loss 4.0764\n",
            "epoch 4 iteration 8000/11250 loss 4.0890\n",
            "=====================================\n",
            "입력:저는 기업에 대해 넓은 지식을 가지고 있지 않기 때문에 제가 아는 기업인 애플과 구글에 투자했다고 가정해 보았습니다 .\n",
            "출력(teacher forcing):I I have n't know to as , the , I am of is like a . be in the . I have .\n",
            "출력(greedy decoding):I have to know that the world , and I have to be a person who are not a lot of the same as\n",
            "정답:Since I do n't have much knowledge about companies , I thought it would be safe to invest in companies that I know .\n",
            "=====================================\n",
            "epoch 4 iteration 8400/11250 loss 4.0780\n",
            "epoch 4 iteration 8800/11250 loss 4.0673\n",
            "epoch 4 iteration 9200/11250 loss 4.0839\n",
            "epoch 4 iteration 9600/11250 loss 4.0699\n",
            "epoch 4 iteration 10000/11250 loss 4.0957\n",
            "=====================================\n",
            "입력:너가 오해할까 봐 말하는데 , 나 너 만나기 전에도 항상 씻고 가고 너랑 헤어지고도 집 오자마자 바로 샤워해 .\n",
            "출력(teacher forcing):I you have always about you have be you I want not you and you have want to letter , you to go you . I you as you\n",
            "출력(greedy decoding):I want to meet you to go to the house , and I will be happy to go to Korea .\n",
            "정답:As I am concerned that you might misunderstand , I am telling you that I always take a shower before going to see you and as soon as I\n",
            "=====================================\n",
            "epoch 4 iteration 10400/11250 loss 4.0624\n",
            "epoch 4 iteration 10800/11250 loss 4.0794\n",
            "epoch 4 iteration 11200/11250 loss 4.0749\n",
            "epoch 5 iteration 400/11250 loss 3.9799\n",
            "epoch 5 iteration 800/11250 loss 4.0007\n",
            "epoch 5 iteration 1200/11250 loss 3.9914\n",
            "epoch 5 iteration 1600/11250 loss 4.0042\n",
            "epoch 5 iteration 2000/11250 loss 3.9992\n",
            "=====================================\n",
            "입력:이건 내 소중한 것을 안에 지니고 있기 때문에 나는 항상 이것을 가지고 있으려고 합니다 .\n",
            "출력(teacher forcing):It 'm to make the with my , the person , I is always precious . .\n",
            "출력(greedy decoding):It is a good idea that I will be happy .\n",
            "정답:I try to keep it with me all the time because it has my valuable inside .\n",
            "=====================================\n",
            "epoch 5 iteration 2400/11250 loss 4.0096\n",
            "epoch 5 iteration 2800/11250 loss 3.9950\n",
            "epoch 5 iteration 3200/11250 loss 3.9924\n",
            "epoch 5 iteration 3600/11250 loss 3.9843\n",
            "epoch 5 iteration 4000/11250 loss 3.9913\n",
            "=====================================\n",
            "입력:모든 제품은 사용자의 편의성을 고려하여 연구 개발되고 있으며 , 지금도 새로운 시도는 진행 중입니다 .\n",
            "출력(teacher forcing):The of the company are currently , the in the the new of the . the , , currently in the .\n",
            "출력(greedy decoding):The product is the same as a new company , and the new system .\n",
            "정답:All of our products are researched and developed by considering the convenience of users and new trials are still in process .\n",
            "=====================================\n",
            "epoch 5 iteration 4400/11250 loss 3.9911\n",
            "epoch 5 iteration 4800/11250 loss 3.9909\n",
            "epoch 5 iteration 5200/11250 loss 3.9950\n",
            "epoch 5 iteration 5600/11250 loss 3.9969\n",
            "epoch 5 iteration 6000/11250 loss 3.9813\n",
            "=====================================\n",
            "입력:저는 많은 나라를 여행하면서 그 나라의 문화를 직접 보고 체험하면서 편견을 하나씩 없애가고 싶습니다 .\n",
            "출력(teacher forcing):I want to introduce a of Korean world of can a Korea countries in the for the a the who the . culture of the .\n",
            "출력(greedy decoding):I want to introduce to travel to the country , and the world .\n",
            "정답:I want to get rid of the stereotypes I have in different countries by looking and experiencing in person by traveling a lot of countries .\n",
            "=====================================\n",
            "epoch 5 iteration 6400/11250 loss 3.9955\n",
            "epoch 5 iteration 6800/11250 loss 3.9897\n",
            "epoch 5 iteration 7200/11250 loss 4.0057\n",
            "epoch 5 iteration 7600/11250 loss 3.9857\n",
            "epoch 5 iteration 8000/11250 loss 3.9933\n",
            "=====================================\n",
            "입력:하지만 내가 아무리 나만 생각한다고 당신이 말해도 , 우리가 함께 가정을 이룬다면 아기도 가질 텐데 당신은 번듯한 직장 있어 ?\n",
            "출력(teacher forcing):But I if I do that you do I the the , what you can you lot . so do be to . much do what n't what\n",
            "출력(greedy decoding):But I do n't want to do you to be a woman who you have to see you .\n",
            "정답:But even though you say that I think of only me , if we make a family , we will have babies too , but do you have any\n",
            "=====================================\n",
            "epoch 5 iteration 8400/11250 loss 3.9720\n",
            "epoch 5 iteration 8800/11250 loss 3.9858\n",
            "epoch 5 iteration 9200/11250 loss 4.0002\n",
            "epoch 5 iteration 9600/11250 loss 3.9811\n",
            "epoch 5 iteration 10000/11250 loss 3.9870\n",
            "=====================================\n",
            "입력:그것은 그녀가 가진 당당함과 우아함 , 그리고 여성으로서의 자존감을 보여주는 진정한 가치가 담겨 있었어 .\n",
            "출력(teacher forcing):It was have like and , but is and she is of the , a person who the person .\n",
            "출력(greedy decoding):It was a person who is a person who is the person who is the best to the person who\n",
            "정답:I could feel her nobleness , grace , and real value showing pride as a woman in the speech .\n",
            "=====================================\n",
            "epoch 5 iteration 10400/11250 loss 3.9806\n",
            "epoch 5 iteration 10800/11250 loss 3.9909\n",
            "epoch 5 iteration 11200/11250 loss 3.9962\n",
            "epoch 6 iteration 400/11250 loss 3.9023\n",
            "epoch 6 iteration 800/11250 loss 3.8978\n",
            "epoch 6 iteration 1200/11250 loss 3.9200\n",
            "epoch 6 iteration 1600/11250 loss 3.8891\n",
            "epoch 6 iteration 2000/11250 loss 3.9181\n",
            "=====================================\n",
            "입력:저를 포함하여 성인 남자 3명이 숙박할 예정인데 객실 내의 온천이 야외로 되어있는 객실에서 숙박하고 싶습니다 .\n",
            "출력(teacher forcing):I the want the in be in for so I have to number with the apartment account in the same and\n",
            "출력(greedy decoding):I want to take a place to the car and the same car .\n",
            "정답:Three adults including me will stay there , and we want a room with an outdoor spa in the room .\n",
            "=====================================\n",
            "epoch 6 iteration 2400/11250 loss 3.9193\n",
            "epoch 6 iteration 2800/11250 loss 3.9046\n",
            "epoch 6 iteration 3200/11250 loss 3.9226\n",
            "epoch 6 iteration 3600/11250 loss 3.9175\n",
            "epoch 6 iteration 4000/11250 loss 3.9020\n",
            "=====================================\n",
            "입력:모든 걱정과 스트레스를 잠시 잊을 수 있는 순간 , 우리는 이 시간을 더욱더 달콤한 휴식으로 만들고자 연구합니다 .\n",
            "출력(teacher forcing):I way , you have be a to the time , the , we can hard be a time . a . .\n",
            "출력(greedy decoding):All the things can be able to make a lot of the best to do .\n",
            "정답:The moment when we could be free from every worry and stress , we study to make this time for sweeter rest .\n",
            "=====================================\n",
            "epoch 6 iteration 4400/11250 loss 3.9249\n",
            "epoch 6 iteration 4800/11250 loss 3.9187\n",
            "epoch 6 iteration 5200/11250 loss 3.9150\n",
            "epoch 6 iteration 5600/11250 loss 3.8956\n",
            "epoch 6 iteration 6000/11250 loss 3.9345\n",
            "=====================================\n",
            "입력:널 제명하기 전 우리는 우리 규칙에 대해서 충분히 설명했고 네가 그것을 잘 알고 있다는 걸 알아요 .\n",
            "출력(teacher forcing):I have know that mind . that first . I have that can have it . much .\n",
            "출력(greedy decoding):I know what you want to be a problem with you .\n",
            "정답:We thoroughly explained our rules before the expulsion and we believe you already understood them very well .\n",
            "=====================================\n",
            "epoch 6 iteration 6400/11250 loss 3.9210\n",
            "epoch 6 iteration 6800/11250 loss 3.9128\n",
            "epoch 6 iteration 7200/11250 loss 3.9149\n",
            "epoch 6 iteration 7600/11250 loss 3.9198\n",
            "epoch 6 iteration 8000/11250 loss 3.9238\n",
            "=====================================\n",
            "입력:만약 당신이 주문 수량을 더 늘릴 경우 , 당신에게 더 낮은 가격으로 공급하는 것이 가능합니다 .\n",
            "출력(teacher forcing):If you need the product , , you can more to the can use more to you price price .\n",
            "출력(greedy decoding):If you can use the price , we can be more discount , we can use more than the price\n",
            "정답:If you increase your order quantity , it is possible for you to be provided with a lower price .\n",
            "=====================================\n",
            "epoch 6 iteration 8400/11250 loss 3.9321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ExFwHofUetQs"
      },
      "source": [
        "#결과값 출력\n",
        "model.eval()\n",
        "for idx, batch in enumerate(train_loader):\n",
        "    x, y, x_len, y_len = batch\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    pred = model.greedy_decoding(x)\n",
        "\n",
        "    for i in range(pred.shape[0]):\n",
        "        pred_indices = pred[i].data.cpu().tolist()\n",
        "        input_indices = x[i].data.cpu().tolist()\n",
        "        label_indices = y[i].data.cpu().tolist()\n",
        "\n",
        "        if 2 in pred_indices:\n",
        "            pred_len = pred_indices.index(2)\n",
        "        else:\n",
        "            pred_len = 128\n",
        "\n",
        "        if 2 in input_indices:\n",
        "            input_len = input_indices.index(2)\n",
        "        else:\n",
        "            input_len = 128\n",
        "        \n",
        "        if 2 in label_indices:\n",
        "            label_len = label_indices.index(2)\n",
        "        else:\n",
        "            label_len = 128\n",
        "        \n",
        "        pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "        input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "        label_words = [english_id2word[idx] for i,idx in enumerate(label_indices) if i <= label_len]\n",
        "        if pred_len>1:\n",
        "            print('===================')\n",
        "            print('입력:{}'.format(' '.join(input_words)))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(pred_words)))\n",
        "            print('답안:{}'.format(' '.join(label_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lnti33cetQt"
      },
      "source": [
        "# model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2E8EQ4jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w97VaEnaetQt"
      },
      "source": [
        "## model load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jol3wa3jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTkUDvtietQt"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbiBFitsetQu"
      },
      "source": [
        "def test_pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x.shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x.shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [pad_tensor(x, pad=src_max_len, dim=dim) for x in batch]\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack(batch, dim=0)\n",
        "    return xs.long(), sequence_lengths.int()\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self,  ko_path, ko_word2id):\n",
        "        self.load_data(ko_path)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        \n",
        "        \n",
        "    def load_data(self, ko_path):\n",
        "        korean_test_data_ = pd.read_csv(\"./korean_data/test_korean.csv\")\n",
        "        korean_test_data =korean_test_data_['Korean'].iloc[:2000].values\n",
        "        korean_test_data_token = []\n",
        "        for sent in korean_test_data:\n",
        "            korean_test_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "        korean_test_data_token = [' '.join(token) for token in korean_test_data_token]\n",
        "        self.ko_seqs = korean_test_data_token\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        return ko_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"])\n",
        "        words = seq.strip().split(' ')\n",
        "        for word in words:\n",
        "            if word in word2id:\n",
        "                sequence.append(word2id[word]) # \n",
        "            else:\n",
        "                sequence.append(3) # replace by <unk> token\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence\n",
        "    \n",
        "    \n",
        "def test(model):\n",
        "    \n",
        "    with open(\"./korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    \n",
        "    test_dataset = TestDataset('./test_korean.pkl', korean_word2id)\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=model_config.batch_size, shuffle=False, collate_fn=test_pad_collate, drop_last=False)\n",
        "    model.eval()\n",
        "    j = 0\n",
        "    \n",
        "    f = open('prediction_result.txt', 'w')\n",
        "    for idx, batch in enumerate(test_loader):\n",
        "        x, x_len= batch\n",
        "        x = x.cuda()\n",
        "        pred = model.greedy_decoding(x)\n",
        "\n",
        "        for i in range(pred.shape[0]):\n",
        "            j+=1\n",
        "            \n",
        "            pred_indices = pred[i].data.cpu().tolist()\n",
        "            input_indices = x[i].data.cpu().tolist()\n",
        "\n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "            input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "            \n",
        "            if j%50 == 0 :\n",
        "                print('========== index {} ========='.format(j))\n",
        "                print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "                print('출력:{}'.format(' '.join(pred_words[:-1])))\n",
        "\n",
        "            f.write(' '.join(pred_words[:-1]))\n",
        "            f.write('\\n')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Em4iwSp8etQu"
      },
      "source": [
        "test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EjRI_zQetQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 과제\n",
        "1. query, key, value 간의 관계에 대해 작성해주세요.\n",
        "A.일단 각각의 의미를 먼저 정의하겠습니다.\n",
        "query는 현재 처리하고자 하는 token을 나타내는 vector\n",
        "key는 일종의 label, 시퀀스 내에 있는 모든 토큰에 대한 identity\n",
        "value는 Key와 연결된 실제 토큰을 나타내는 vector이다.\n",
        "이들 간의 관계는 self-attention을 할때 이 요소들의 행렬들을 계산을 한다.\n",
        "2. multi-head 의 필요성에 대해 작성해주세요.\n",
        "A.만약 head 1개로 돌아가게 되면 문장을 여러 측면에서 고려하지 못하게 되는 문제가 발생한다. 에를 들어 문장에서 누가, 언제, 어디서 등의 정보가 존재할텐데 multi head attention을 두면 각 attention이 누가, 언제, 어디서를 각각 집중적으로 담당할 수 있게 될 것이다.\n",
        "3. BERT 사전 학습의 두가지 태스크에 대해 설명해주세요.\n",
        "A. MLM(Masked Language Model): 입력 문장에서 임의로 토큰을 버리고(Mask), 그 토큰을 맞추는 방식으로 학습을 진행한다.\n",
        "NSP(Next Sentence Prediction) :두 문장이 주어졌을 때, 두 문장의 순서를 예측하는 방식입니다. 두 문장 간 관련이 고려되야 하는 NLI와 QA의 파인 튜닝을 위해 두 문장의 연관을 맞추는 학습을 진행한다.\n",
        "4. Transformer embedding과 BERT embedding의 차이점 두가지를 작성해주세요.\n",
        "A.BERT는 Transformer와 달리 Positional Encoding을 사용하지 않고 대신 Positional Embeddings를 사용합니다. 여기에 Segment Embeddings를 추가해 각각의 임베딩, 즉 3개의 임베딩을 합산한 결과를 취한다.\n"
      ],
      "metadata": {
        "id": "4czqG7nTxnI_"
      }
    }
  ]
}