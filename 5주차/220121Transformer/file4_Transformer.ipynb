{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "file4_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFyZ5vetQb"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYCLJa4petQg"
      },
      "source": [
        "def split_last(x, shape):\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQR9lBMetQh"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295zs7CWetQj"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWS8voZZetQk"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYgHC0ietQk"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # sequence 방향 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # sequence 방향 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiPES9getQl"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbTPl6aSetQm"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table) # (S, D)\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embed = nn.Embedding(vocab_size, cfg.dim) # token embedding\n",
        "\n",
        "        # from_pretrained는 사전에 정의된 것들을 쓰겠다는 의미 # freeze=True는 학습을 안시키고 고정시키겠다라는 의미\n",
        "        self.pos_embed = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(512, cfg.dim),freeze=True) # position embedding\n",
        "\n",
        "\n",
        "        # seg embbed가 없다.\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsNAdbnetQn"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ulMozSetQn"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1)) # scale\n",
        "        \n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDtvhua3etQo"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    # x_q=None는 쿼리는 다른곳(디코더)에서 오기 때문에\n",
        "    def forward(self, x, mask, x_q=None):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        \n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        if x_q is None:\n",
        "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        else:\n",
        "            q, k, v = self.proj_q(x_q), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "            scores = scores.masked_fill_(mask, -1e9)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = torch.matmul(scores, v).transpose(1,2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQUnmexetQo"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMI83XbetQo"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgFSAOGNetQp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEoqWUFetQp"
      },
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "\n",
        "#1. pad mask (attention할 때 씀, 왜? pad attention score 안쓰겠다.)\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size() # [B, S_q]\n",
        "    batch_size, len_k = seq_k.size() # [B, S_k] \n",
        "\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "\n",
        "# 2. subsequent_mask : 디코더에서 Masked Multi Head Attention 할때 사용. 왜?: 컨닝 못하게 하려고\n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    # 우리는 Q, K^T 곱한 것의 upper tirangle 을 masking할 것이다.\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    # 0 1 1\n",
        "    # 0 0 1\n",
        "    # 0 0 0\n",
        "    subsequent_mask = torch.tensor(subsequent_mask, device=seq.device).byte() # torch tensor로 만들어줌\n",
        "    return subsequent_mask\n",
        "    \n",
        "    \n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(cfg)\n",
        "        self.encoder_attention = MultiHeadAttention(cfg)\n",
        "        \n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.proj1 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.proj2 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        \n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm3 = LayerNorm(cfg)\n",
        "        \n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        \n",
        "    def forward(self,x , enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \n",
        "        # self-attention -> add&norm\n",
        "        h = self.self_attention(x, dec_self_attn_mask)\n",
        "        h = self.norm1(x + self.drop(self.proj1(h)))\n",
        "        \n",
        "        # encoder attention -> add&norm\n",
        "        h2 = self.self_attention(enc_outputs, dec_enc_attn_mask, x_q=h)\n",
        "        h = self.norm2(h + self.drop(self.proj2(h2))) \n",
        "        \n",
        "        # feedforward network -> add&norm\n",
        "        h = self.norm3(h + self.drop(self.pwff(h)))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        #====================encoder===========================\n",
        "        self.encoder_embed = Embeddings(cfg, len(korean_vocab))\n",
        "        self.encoder_blocks = nn.ModuleList([Encoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        #====================decoder============================\n",
        "        self.decoder_embed = Embeddings(cfg, len(english_vocab))\n",
        "        self.decoder_blocks = nn.ModuleList([Decoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "        \n",
        "        #=========================================================\n",
        "        self.projection = nn.Linear(cfg.dim, len(english_vocab))\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        #============encoder============\n",
        "        h = self.encoder_embed(enc_inputs)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h, enc_self_attn_mask)\n",
        "            \n",
        "        enc_outputs = h\n",
        "        \n",
        "        \n",
        "        #============decoder============\n",
        "        \n",
        "        # self attention mask\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).float()\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs).float()\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        # encoder attention mask\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        \n",
        "        \n",
        "        # embedding\n",
        "        h = self.decoder_embed(dec_inputs)\n",
        "        \n",
        "        \n",
        "        for block in self.decoder_blocks:\n",
        "            h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "        #============projection==========\n",
        "        \n",
        "        out = self.projection(h)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def greedy_decoding(self, enc_inputs, start_token_index = 1, end_token_index = 2, generation_max_len=128):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            batch_size, max_length = enc_inputs.size()\n",
        "            generation_end_flag = [0 for i in range(batch_size)]\n",
        "            predicted_sentences = []\n",
        "            #=================encoding=============\n",
        "            h = self.encoder_embed(enc_inputs)\n",
        "            enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "            for block in self.encoder_blocks:\n",
        "                h = block(h, enc_self_attn_mask)    \n",
        "            enc_outputs = h\n",
        "\n",
        "            #================ greedy decoding ==================\n",
        "            # dec_inputs : (batch size, 1) \n",
        "            dec_inputs = torch.ones(batch_size, 1, device=enc_inputs.device) * start_token_index\n",
        "            dec_inputs = dec_inputs.long()\n",
        "\n",
        "            for i in range(generation_max_len):\n",
        "\n",
        "                #====================== decoder =======================\n",
        "                # self attention mask\n",
        "                dec_self_attn_pad_mask = None\n",
        "                dec_self_attn_subsequent_mask = None\n",
        "                dec_self_attn_mask = None\n",
        "\n",
        "                # encoder attention mask\n",
        "                dec_enc_attn_mask = None\n",
        "\n",
        "\n",
        "                # embedding\n",
        "                h = self.decoder_embed(dec_inputs)\n",
        "\n",
        "\n",
        "                for block in self.decoder_blocks:\n",
        "                    h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "\n",
        "\n",
        "                out = self.projection(h[:,-1,:])\n",
        "                pred = out.argmax(-1) \n",
        "\n",
        "#                 print(out.size(), pred)\n",
        "\n",
        "                dec_inputs = torch.cat((dec_inputs, pred.unsqueeze(1)),dim=1)\n",
        "    #             print(dec_inputs)\n",
        "\n",
        "                predicted_sentences.append(pred)\n",
        "                for j, boolean in enumerate(pred==end_token_index):\n",
        "                    if boolean == True:\n",
        "                        generation_end_flag[j] = 1\n",
        "                if sum(generation_end_flag) == batch_size:\n",
        "                    break\n",
        "\n",
        "        return torch.stack(predicted_sentences, dim=1)\n",
        "        \n",
        "        "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8DHAo9QetQu"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}