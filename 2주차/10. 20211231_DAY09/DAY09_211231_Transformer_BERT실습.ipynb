{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer&BERT실습.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmCgeYtGjFtPdUd19DwiGa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install tokenizers"],"metadata":{"id":"vl6qGf1GUVrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","import time\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"EW5ps47KOPPT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer로 챗봇을 만들어봅시다!\n","- Transformer에 문장을 입력하면, 그에 대한 대답을 하는 챗봇을 만들 것입니다.\n","- 중간중간 빈칸을 채워넣어가며 자신만의 챗봇 코드를 완성해보세요."],"metadata":{"id":"jOGAzogcSsmo"}},{"cell_type":"markdown","source":["우선 챗봇 학습을 위한 데이터를 다운로드 받습니다."],"metadata":{"id":"Ew9i4J_ATGbW"}},{"cell_type":"code","source":["# data download\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","train_data = pd.read_csv('ChatBotData.csv')"],"metadata":{"id":"mHZkvZGmSuyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('데이터 샘플의 개수 :', len(train_data))"],"metadata":{"id":"6ojYsR2aS9ag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head(10)"],"metadata":{"id":"rh_q-eifTDiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 한국어 데이터를 전처리하는 방법에 대해 알아봅시다.\n","- 우리는 네이버 영화리뷰 데이터에 tokenizer를 학습시켜 단어를 구분하도록 할 예정입니다. \n","(챗봇 데이터에 비해 훨씬 많은 데이터를 포함하고 있습니다.)"],"metadata":{"id":"agVQBr16TZHR"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n","naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how='any')\n","with open('naver_review.txt', 'w', encoding='utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"asKi2NoTTL0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tokenizers import BertWordPieceTokenizer\n","tokenizer = BertWordPieceTokenizer(lowercase=False) # lowercase를 구분할지 여부를 선택합니다 (True: 대문자 무시(모두 소문자로 인식), False:대소문자 구분)\n"],"metadata":{"id":"_Ksuv9T9T3KP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = 'naver_review.txt'\n","vocab_size = 30000\n","limit_alphabet = 6000\n","min_frequency = 5\n","\n","tokenizer.train(files=data_file,\n","                vocab_size=vocab_size,\n","                limit_alphabet=limit_alphabet,\n","                min_frequency=min_frequency,\n","                special_tokens=['[PAD]', '[START]', '[END]', '[UNK]'])"],"metadata":{"id":"CgwAEodSUPSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocab 저장\n","tokenizer.save_model('./')\n","vocab_df = pd.read_fwf('vocab.txt', header=None)\n","vocab_df"],"metadata":{"id":"mmbXucF5Uu3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챗봇 모델은 \"정수 인코딩\" 결과를 사용하여 모델을 작동시키고, 최종 예측 결과도 정수로 인코딩 된 결과를 내뱉습니다.\n","# 인코딩된 결과물을 decode 를 통해 해석 가능한 문장으로 바꿔줄 수 있습니다.\n","encoded = tokenizer.encode('챗봇이 잘 완성될까요?')\n","print('토큰화 결과 :',encoded.tokens)\n","print('정수 인코딩 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"id":"na8cPqVZVNB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IxK16jsQXOzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer 모델을 만들어봅시다."],"metadata":{"id":"_KxCM1IiVtbA"}},{"cell_type":"code","source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        # ============== TODO ======================\n","        # torch.nn 에는 Transformer 모델이 이미 구현되어 있습니다. \n","        # 이것을 이용해 챗봇 모델을 완성해봅시다.\n","        # ==========================================\n","        self.transformer = ????(d_model=emb_size,\n","                                nhead=nhead,\n","                                num_encoder_layers=num_encoder_layers,\n","                                num_decoder_layers=num_decoder_layers,\n","                                dim_feedforward=dim_feedforward,\n","                                dropout=dropout)\n","        # Transformer의 output값을 vocabulary의 index를 나타내는 벡터로 바꾸어주는 선형 변환이 필요합니다.\n","        self.generator = ????(emb_size, tgt_vocab_size) \n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        # ============== TODO ======================\n","        # 위에서 구현한 트랜스포머 모델에 입력된 데이터를 통과시켜봅시다.\n","        # ==========================================\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.????(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.????(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"metadata":{"id":"sML-QsR1VSiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Transformer의 학습 중에는, 미래의 데이터를 볼 수 없도록 하는 마스크가 필요합니다."],"metadata":{"id":"fGoPx7h4Wtxe"}},{"cell_type":"code","source":["PAD_IDX = 0\n","def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"metadata":{"id":"se0BOkYxWptG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC_VOCAB_SIZE = vocab_size # 30000\n","TGT_VOCAB_SIZE = vocab_size # 30000\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","# 모델을 initialize 해줍니다.\n","model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","model = model.to(DEVICE) # model 을 GPU로 보내줍니다.\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=????) # ignore_index 가 하는 역할은 무엇일까요?\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"Z1mFnKa_W46Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","class ChatbotDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x, y, _ = self.data[idx]\n","        # ============== TODO ==============\n","        # 1. 데이터를 토크나이즈 해보세요. (x,y 모두)\n","        # 2. start token 과 end token을 추가해보세요. (x,y 모두)\n","        # 3. 토큰들을 torch tensor (long 타입)로 변환하고 return하세요.\n","        # ==================================\n","\n","        x_tokens = ????\n","        y_tokens = ????\n","\n","        # hint: x_tokens 와 y_tokens 는 list type입니다.\n","        x_tokens.????\n","        x_tokens.????\n","        y_tokens.????\n","        y_tokens.????\n","\n","        return ????, ????"],"metadata":{"id":"yrdvopXGXene"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = ChatbotDataset(train_data.values, tokenizer)"],"metadata":{"id":"zV77ccjfYaSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[0]"],"metadata":{"id":"S-2OOeplYe1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collate_fn 을 정의해줍니다.\n","# collate_fn 은 dataset으로부터 여러 item을 받아와 하나의 배치로 합칠 때, 어떻게 합칠지를 정의하는 부분입니다.\n","\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","def collate_fn(batch):\n","    # batch: [(x1, y1), (x2, y2), ... ]\n","    # ============== TODO ==============\n","    # 우리의 목표는 X = [x1,x2,x3,...] , Y = [y1, y2, y3, ...] 형태의 tensor로 만드는 것입니다.\n","    # 이를 위해 2가지 해야할 일이 있습니다.\n","    # 1. batch 안에 있는 x 와 y를 각각의 list 에 모아주기.\n","    # 2. padding을 통해 동일한 길이로 만들어주고, 하나의 tensor로 통합하기.\n","    # 아래 빈칸을 채워 위 두가지를 진행해보세요.\n","    # ==================================\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(src_sample)\n","        tgt_batch.append(tgt_sample)\n","    \n","    # padding_value는 pad 위치에 어떤 값을 넣을지를 정하는 값입니다.\n","    src_batch = ????(src_batch, padding_value=????)\n","    tgt_batch = ????(tgt_batch, padding_value=????) \n","    return src_batch, tgt_batch\n","\n","# dataloader 를 정의해줍니다.\n","batch_size = 32\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n","                         num_workers=2, collate_fn=collate_fn,\n","                          pin_memory=True, drop_last=True)"],"metadata":{"id":"Ghm16BskYotq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x,y = next(iter(data_loader))"],"metadata":{"id":"djH7TslZZYnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"D6og4pAHZrTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[:,1]"],"metadata":{"id":"LBJbclAJZv1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer, loss_fn, train_dataloader):\n","    model.train()\n","    losses = 0\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1,:] # 왜 target은 마지막 하나를 빼고 입력할까요?\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :] # 왜 첫번째 단어는 빼고 로스를 계산할까요?\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)"],"metadata":{"id":"vT3K6ov3Z5PX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 10\n","for i in range(EPOCHS):\n","    epoch_loss = train_epoch(model, optimizer, loss_fn, data_loader)\n","    print('EPOCH {} LOSS {:.6f}'.format(i+1, epoch_loss))"],"metadata":{"id":"8ZQY8en1cRtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == 2:\n","            break\n","    return ys\n","\n","def predict(model, tokenizer, src_sentence):\n","    model.eval()\n","    src = tokenizer.encode(src_sentence).ids\n","    src.insert(0,1)\n","    src.append(2)\n","    src = torch.tensor(src).long().unsqueeze(1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=1).flatten()\n","    return tokenizer.decode(tgt_tokens.cpu().tolist())"],"metadata":{"id":"8SgrMYEtcnmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'반가워')"],"metadata":{"id":"fHidx9_zh2A4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model,tokenizer,'여행 가고 싶다.')"],"metadata":{"id":"NozJbX0Ohzzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT 실습\n","- BERT는 Transformer의 encoder를 사용합니다.\n","- Transformer 구현에 대해 이미 알아보았으니, BERT를 직접 구현하지 않고 huggingface 라이브러리를 통해 간단하게 구현하는 방법에 대해 배우도록 하겠습니다.\n","- BERT를 이용해서 naver 영화리뷰 데이터의 점수를 분류하는 task를 진행해보겠습니다.\n","- https://huggingface.co/"],"metadata":{"id":"HLuBmGaOj6ie"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"42tc6KpNj55I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 영화리뷰 데이터에 대한 fine-tuning에 앞서, 간단하게 huggingface 라이브러리의 사용방법에 대해 익혀보도록 합시다.\n","\n","- https://huggingface.co/transformers/v3.0.2/index.html"],"metadata":{"id":"2NQkArhXmmLU"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForTokenClassification\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n","\n","inputs = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","outputs = model(inputs)"],"metadata":{"id":"GwEcFu13j0Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"id":"zxvaMZq3kis4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs.shape"],"metadata":{"id":"tcUtbtqYkzwj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 사전 학습된 BERT의 mask 토큰에 대한 예측 결과를 확인해봅시다."],"metadata":{"id":"XfYg-tHWzwZP"}},{"cell_type":"code","source":["from transformers import pipeline\n","unmasker = pipeline('fill-mask', model='bert-base-uncased')\n"],"metadata":{"id":"96WGnTcHlFKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unmasker(\"The man worked as a [MASK].\")"],"metadata":{"id":"b2aqWKpelG7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unmasker(\"The woman worked as a [MASK].\")"],"metadata":{"id":"wsTfUC_wlHGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unmasker(\"I have a [MASK].\")"],"metadata":{"id":"OnZZMGx_lK0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- BERT tokenizer와 model을 이용해 네이버 영화리뷰 데이터에 fine-tuning을 진행해봅시다."],"metadata":{"id":"ga1ACldFm5Mi"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n","naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how='any')\n","with open('naver_review.txt', 'w', encoding='utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"lJMvSTRHlLNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["naver_df.head()"],"metadata":{"id":"3__KVAxBnF_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["naver_df.tail()"],"metadata":{"id":"9IurulqbnKND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["naver_df['label'].unique()"],"metadata":{"id":"iFabXNApnOpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased') # multilingual-BERT를 사용해보겠습니다.\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n","\n","inputs = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","outputs = model(inputs)"],"metadata":{"id":"29omBRwInCyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"id":"0l6yziPznhaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label을 입력해주면 classification에 대한 loss도 자동으로 계산할 수 있습니다.\n","model(inputs, labels=torch.tensor([1]))"],"metadata":{"id":"TzQn2AynoOVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_idx = np.random.choice(range(len(naver_df)), size=len(naver_df)//5*4, replace=False)"],"metadata":{"id":"mPuHYLyaoYzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_idx"],"metadata":{"id":"uQ-HaWScpGsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = naver_df.iloc[train_data_idx][['document','label']].values\n","test_data = naver_df.iloc[~naver_df.index.isin(train_data_idx)][['document','label']].values"],"metadata":{"id":"vQ2JXRmyouDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","class ReviewDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x, y = self.data[idx]\n","        # BERT tokenizer는 batch 단위로 한번에 token화 할 수 있습니다.\n","        # dataloder에서 얻어온 후 한번에 tokenize 해보도록 하겠습니다.\n","        # text는 list로, label은 long tensor로 리턴해줍니다.\n","        return x, torch.tensor(y).long()"],"metadata":{"id":"GSY_BLG6patJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = ReviewDataset(train_data)\n","test_dataset = ReviewDataset(test_data)"],"metadata":{"id":"XKdKtfWSqBXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n","                         num_workers=2, collate_fn=None,\n","                          pin_memory=True, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n","                         num_workers=2, collate_fn=None,\n","                          pin_memory=True, drop_last=False)"],"metadata":{"id":"X4nbsudhqJmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y = next(iter(train_loader))"],"metadata":{"id":"qqAq-aS5qWY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"AxxRdXc9qY3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"I8F4FOE-qiN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_x = tokenizer.batch_encode_plus(x, padding=True, return_tensors='pt')"],"metadata":{"id":"9tXTAfooqkEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_x.keys()"],"metadata":{"id":"z4f3FkTmq9rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_x['input_ids']"],"metadata":{"id":"OhmJc8C_q_Ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_x['token_type_ids']"],"metadata":{"id":"9RaEp2z_rYOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_x['attention_mask']"],"metadata":{"id":"5M0l0JNLrc60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, dataloader, tokenizer, optimizer):\n","    model.train()\n","    train_loss = 0\n","    for i, (x,y) in enumerate(dataloader):\n","        # =========== TODO ============\n","        # 위에서 배운 내용을 바탕으로 빈칸을 채워보세요!\n","        # =============================\n","        x = tokenizer.????(x, padding=True, return_tensors='pt')['input_ids'].to(DEVICE)\n","        y = y.to(DEVICE)\n","        loss = model(x, labels=y)[????]\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        if i % 50 == 0:\n","            print('Iter [{}/{}] Loss {:.6f}'.format(i+1, len(dataloader), train_loss / (i+1)))\n","    \n","    return train_loss / len(dataloader)\n","\n","def test_epoch(model, dataloader, tokenizer):\n","    model.eval()\n","    preds = []\n","    labels = []\n","    with torch.no_grad():\n","      for x,y in dataloader:\n","          # =========== TODO ============\n","          # 위에서 배운 내용을 바탕으로 빈칸을 채워보세요!\n","          # =============================\n","          x = tokenizer.????(x, padding=True, return_tensors='pt')['input_ids'].to(DEVICE)\n","          out = model(x)[????]\n","          pred = out.argmax(-1)\n","          preds.append(pred.cpu())\n","          labels.append(y)\n","    preds = torch.cat(preds)\n","    labels = torch.cat(labels)\n","    acc = (preds == labels).float().mean()\n","    print('ACC : {:.3f}'.format(acc))\n","    return preds, labels\n","\n","def predict(model, tokenizer, sentence):\n","    model.eval()\n","    x = tokenizer.encode(sentence, return_tensors='pt').to(DEVICE)\n","    out = model(x)['logits']\n","    pred = out.argmax(-1)\n","    return pred.cpu()"],"metadata":{"id":"3ZTJ1rehrgBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n","model = model.to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"],"metadata":{"id":"clUygW4uvkX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS=1\n","\n","for i in range(EPOCHS):\n","    train_epoch(model, train_loader, tokenizer, optimizer)\n","    test_epoch(model, test_loader, tokenizer)"],"metadata":{"id":"0kFhDWeTr5HM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model, tokenizer, '이 영화는 최고야')"],"metadata":{"id":"vfGloJpUv8b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict(model, tokenizer, '이 영화는 별로야')"],"metadata":{"id":"XQe0g7Y4sIlf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"si6-7RKBvYju"},"execution_count":null,"outputs":[]}]}